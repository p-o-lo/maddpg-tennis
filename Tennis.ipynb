{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the third project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Tennis.app\"`\n",
    "- **Windows** (x86): `\"path/to/Tennis_Windows_x86/Tennis.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Tennis_Windows_x86_64/Tennis.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Tennis_Linux/Tennis.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Tennis_Linux/Tennis.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Tennis.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Tennis.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"Tennis_Linux/Tennis.x86_64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1.  If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01.  Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.65278625 -1.5\n",
      " -0.          0.          6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. The MADDPG Solution\n",
    "\n",
    "See the commented file `multi_ddpg.py` and `model.py` for more details. Essentially is implemented the same algorithm of the original [MADDPG paper](https://arxiv.org/pdf/1706.02275.pdf).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import files\n",
    "from multi_ddpg import Agent\n",
    "import torch\n",
    "from collections import OrderedDict, namedtuple, deque\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instance of the multi ddpg agents (in this context 2 agents are instanciated )\n",
    "agent = Agent(state_size, action_size, random_seed=2)\n",
    "\n",
    "### Training loop\n",
    "def run_ddpg_multi(n_episodes=5000, queue=100):        \n",
    "    \"\"\"Multi Deep Deterministic Policy Gradient learning for Tennis Unity Environment.\n",
    "    \n",
    "    Params Input\n",
    "    ==========\n",
    "        n_episode (int): maximum number of episodes\n",
    "        queue (int): number of consecutive episodes to average up\n",
    "        \n",
    "    Params Output\n",
    "    ==========\n",
    "        scores_all (list of floats): are the scores collected at the end of each episode\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    ##Inizialization\n",
    "    scores_window = deque(maxlen=queue)     \n",
    "    scores_all = []                         \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]      \n",
    "        states = env_info.vector_observations\n",
    "        agent.reset()\n",
    "        scores = np.zeros(2)\n",
    "        \n",
    "        ## Training loop of each episode\n",
    "        while True:\n",
    "            actions = agent.act(states, add_noise=True)\n",
    "            env_info = env.step(actions)[brain_name]           \n",
    "            next_states = env_info.vector_observations         \n",
    "            rewards = env_info.rewards                         \n",
    "            dones = env_info.local_done\n",
    "            agent.step(states, actions, rewards, next_states, dones)\n",
    "            scores += rewards                                  \n",
    "            states = next_states                               \n",
    "            if np.any(dones):                                  \n",
    "                break\n",
    "\n",
    "        scores_window.append(np.max(scores))\n",
    "        scores_all.append(np.max(scores))\n",
    "        \n",
    "        print('Episode {} ## Reward 1 agent: {:.3f} ##  Reward 2 : {:.3f} ## Max: {:.3f}'.format(\n",
    "                i_episode, scores[0], scores[1], np.max(scores)))\n",
    "        print('Average Score of last 100 episodes: {:.3f}'.format(np.mean(scores_window) ))\n",
    "\n",
    "        if np.mean(scores_window)>=0.5:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.3f}'.format(\n",
    "                i_episode, np.mean(scores_window)))\n",
    "            torch.save(agent.actor_local0.state_dict(), 'checkpoint_actor0.pth')\n",
    "            torch.save(agent.critic_local0.state_dict(), 'checkpoint_critic0.pth')\n",
    "            torch.save(agent.actor_local1.state_dict(), 'checkpoint_actor1.pth')\n",
    "            torch.save(agent.critic_local1.state_dict(), 'checkpoint_critic1.pth')\n",
    "            break\n",
    "\n",
    "    return scores_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Run and plot of the outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.000\n",
      "Episode 2 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.000\n",
      "Episode 3 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.000\n",
      "Episode 4 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.000\n",
      "Episode 5 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.000\n",
      "Episode 6 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.000\n",
      "Episode 7 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.000\n",
      "Episode 8 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.000\n",
      "Episode 9 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.000\n",
      "Episode 10 ## Reward 1 agent: 0.100 ##  Reward 2 : 0.090 ## Max: 0.100\n",
      "Average Score of last 100 episodes: 0.010\n",
      "Episode 11 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.009\n",
      "Episode 12 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.008\n",
      "Episode 13 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.008\n",
      "Episode 14 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.007\n",
      "Episode 15 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.007\n",
      "Episode 16 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.006\n",
      "Episode 17 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.006\n",
      "Episode 18 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.006\n",
      "Episode 19 ## Reward 1 agent: 0.100 ##  Reward 2 : -0.010 ## Max: 0.100\n",
      "Average Score of last 100 episodes: 0.011\n",
      "Episode 20 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.010\n",
      "Episode 21 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.010\n",
      "Episode 22 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.009\n",
      "Episode 23 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.009\n",
      "Episode 24 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.008\n",
      "Episode 25 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.008\n",
      "Episode 26 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.100 ## Max: 0.100\n",
      "Average Score of last 100 episodes: 0.012\n",
      "Episode 27 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.011\n",
      "Episode 28 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.011\n",
      "Episode 29 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.010\n",
      "Episode 30 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.010\n",
      "Episode 31 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.010\n",
      "Episode 32 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.009\n",
      "Episode 33 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.009\n",
      "Episode 34 ## Reward 1 agent: 0.100 ##  Reward 2 : -0.010 ## Max: 0.100\n",
      "Average Score of last 100 episodes: 0.012\n",
      "Episode 35 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.011\n",
      "Episode 36 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.011\n",
      "Episode 37 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.011\n",
      "Episode 38 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.011\n",
      "Episode 39 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.010\n",
      "Episode 40 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.010\n",
      "Episode 41 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.010\n",
      "Episode 42 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.010\n",
      "Episode 43 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.009\n",
      "Episode 44 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.009\n",
      "Episode 45 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.009\n",
      "Episode 46 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.009\n",
      "Episode 47 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.009\n",
      "Episode 48 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.008\n",
      "Episode 49 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.008\n",
      "Episode 50 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.008\n",
      "Episode 51 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.008\n",
      "Episode 52 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.008\n",
      "Episode 53 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.008\n",
      "Episode 54 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.007\n",
      "Episode 55 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.007\n",
      "Episode 56 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.007\n",
      "Episode 57 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.007\n",
      "Episode 58 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.007\n",
      "Episode 59 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.007\n",
      "Episode 60 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.007\n",
      "Episode 61 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.007\n",
      "Episode 62 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.006\n",
      "Episode 63 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.006\n",
      "Episode 64 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.006\n",
      "Episode 65 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.006\n",
      "Episode 66 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.006\n",
      "Episode 67 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.006\n",
      "Episode 68 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.006\n",
      "Episode 69 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.006\n",
      "Episode 70 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.006\n",
      "Episode 71 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.006\n",
      "Episode 72 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 73 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.005\n",
      "Episode 74 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.005\n",
      "Episode 75 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.005\n",
      "Episode 76 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.005\n",
      "Episode 77 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.005\n",
      "Episode 78 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.005\n",
      "Episode 79 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.005\n",
      "Episode 80 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.005\n",
      "Episode 81 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.005\n",
      "Episode 82 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.005\n",
      "Episode 83 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.005\n",
      "Episode 84 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.005\n",
      "Episode 85 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.005\n",
      "Episode 86 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.005\n",
      "Episode 87 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.005\n",
      "Episode 88 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.005\n",
      "Episode 89 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.004\n",
      "Episode 90 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.004\n",
      "Episode 91 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.004\n",
      "Episode 92 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.004\n",
      "Episode 93 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.004\n",
      "Episode 94 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.004\n",
      "Episode 95 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.004\n",
      "Episode 96 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.004\n",
      "Episode 97 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.004\n",
      "Episode 98 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.004\n",
      "Episode 99 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.004\n",
      "Episode 100 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.004\n",
      "Episode 101 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.004\n",
      "Episode 102 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.004\n",
      "Episode 103 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.004\n",
      "Episode 104 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.004\n",
      "Episode 105 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.004\n",
      "Episode 106 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.004\n",
      "Episode 107 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.004\n",
      "Episode 108 ## Reward 1 agent: 0.000 ##  Reward 2 : 0.090 ## Max: 0.090\n",
      "Average Score of last 100 episodes: 0.005\n",
      "Episode 109 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.005\n",
      "Episode 110 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.004\n",
      "Episode 111 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.100 ## Max: 0.100\n",
      "Average Score of last 100 episodes: 0.005\n",
      "Episode 112 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.005\n",
      "Episode 113 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.005\n",
      "Episode 114 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.005\n",
      "Episode 115 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.100 ## Max: 0.100\n",
      "Average Score of last 100 episodes: 0.006\n",
      "Episode 116 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.006\n",
      "Episode 117 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.006\n",
      "Episode 118 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.006\n",
      "Episode 119 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.005\n",
      "Episode 120 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.005\n",
      "Episode 121 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.005\n",
      "Episode 122 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.005\n",
      "Episode 123 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.005\n",
      "Episode 124 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.005\n",
      "Episode 125 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.005\n",
      "Episode 126 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.004\n",
      "Episode 127 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.004\n",
      "Episode 128 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.004\n",
      "Episode 129 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.004\n",
      "Episode 130 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.004\n",
      "Episode 131 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.004\n",
      "Episode 132 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.004\n",
      "Episode 133 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.004\n",
      "Episode 134 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.003\n",
      "Episode 135 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.003\n",
      "Episode 136 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.003\n",
      "Episode 137 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.003\n",
      "Episode 138 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.003\n",
      "Episode 139 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.003\n",
      "Episode 140 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.003\n",
      "Episode 141 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.003\n",
      "Episode 142 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.003\n",
      "Episode 143 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.003\n",
      "Episode 144 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 145 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.003\n",
      "Episode 146 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.003\n",
      "Episode 147 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.003\n",
      "Episode 148 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.003\n",
      "Episode 149 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.003\n",
      "Episode 150 ## Reward 1 agent: 0.000 ##  Reward 2 : 0.090 ## Max: 0.090\n",
      "Average Score of last 100 episodes: 0.004\n",
      "Episode 151 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.004\n",
      "Episode 152 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.004\n",
      "Episode 153 ## Reward 1 agent: 0.000 ##  Reward 2 : 0.090 ## Max: 0.090\n",
      "Average Score of last 100 episodes: 0.005\n",
      "Episode 154 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.005\n",
      "Episode 155 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.005\n",
      "Episode 156 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.005\n",
      "Episode 157 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.005\n",
      "Episode 158 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.005\n",
      "Episode 159 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.005\n",
      "Episode 160 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.005\n",
      "Episode 161 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.005\n",
      "Episode 162 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.005\n",
      "Episode 163 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.005\n",
      "Episode 164 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.005\n",
      "Episode 165 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.005\n",
      "Episode 166 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.005\n",
      "Episode 167 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.005\n",
      "Episode 168 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.005\n",
      "Episode 169 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.005\n",
      "Episode 170 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.005\n",
      "Episode 171 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.005\n",
      "Episode 172 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.005\n",
      "Episode 173 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.005\n",
      "Episode 174 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.005\n",
      "Episode 175 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.005\n",
      "Episode 176 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.005\n",
      "Episode 177 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.005\n",
      "Episode 178 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.100 ## Max: 0.100\n",
      "Average Score of last 100 episodes: 0.006\n",
      "Episode 179 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.006\n",
      "Episode 180 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.006\n",
      "Episode 181 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.006\n",
      "Episode 182 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.006\n",
      "Episode 183 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.006\n",
      "Episode 184 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.006\n",
      "Episode 185 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.006\n",
      "Episode 186 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.006\n",
      "Episode 187 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.006\n",
      "Episode 188 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.006\n",
      "Episode 189 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.006\n",
      "Episode 190 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.006\n",
      "Episode 191 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.006\n",
      "Episode 192 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.006\n",
      "Episode 193 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.006\n",
      "Episode 194 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.006\n",
      "Episode 195 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.006\n",
      "Episode 196 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.006\n",
      "Episode 197 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.006\n",
      "Episode 198 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.006\n",
      "Episode 199 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.006\n",
      "Episode 200 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.006\n",
      "Episode 201 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.006\n",
      "Episode 202 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.006\n",
      "Episode 203 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.006\n",
      "Episode 204 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.006\n",
      "Episode 205 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.006\n",
      "Episode 206 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.006\n",
      "Episode 207 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.006\n",
      "Episode 208 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.005\n",
      "Episode 209 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.005\n",
      "Episode 210 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.005\n",
      "Episode 211 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.004\n",
      "Episode 212 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.004\n",
      "Episode 213 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.004\n",
      "Episode 214 ## Reward 1 agent: 0.000 ##  Reward 2 : 0.090 ## Max: 0.090\n",
      "Average Score of last 100 episodes: 0.005\n",
      "Episode 215 ## Reward 1 agent: 0.000 ##  Reward 2 : 0.090 ## Max: 0.090\n",
      "Average Score of last 100 episodes: 0.005\n",
      "Episode 216 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 217 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.005\n",
      "Episode 218 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.005\n",
      "Episode 219 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.005\n",
      "Episode 220 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.005\n",
      "Episode 221 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.005\n",
      "Episode 222 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.005\n",
      "Episode 223 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.005\n",
      "Episode 224 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.005\n",
      "Episode 225 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.005\n",
      "Episode 226 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.005\n",
      "Episode 227 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.100 ## Max: 0.100\n",
      "Average Score of last 100 episodes: 0.006\n",
      "Episode 228 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.006\n",
      "Episode 229 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.006\n",
      "Episode 230 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.006\n",
      "Episode 231 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.006\n",
      "Episode 232 ## Reward 1 agent: 0.000 ##  Reward 2 : 0.090 ## Max: 0.090\n",
      "Average Score of last 100 episodes: 0.007\n",
      "Episode 233 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.007\n",
      "Episode 234 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.007\n",
      "Episode 235 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.007\n",
      "Episode 236 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.007\n",
      "Episode 237 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.007\n",
      "Episode 238 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.007\n",
      "Episode 239 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.007\n",
      "Episode 240 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.007\n",
      "Episode 241 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.007\n",
      "Episode 242 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.007\n",
      "Episode 243 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.007\n",
      "Episode 244 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.007\n",
      "Episode 245 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.007\n",
      "Episode 246 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.007\n",
      "Episode 247 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.007\n",
      "Episode 248 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.007\n",
      "Episode 249 ## Reward 1 agent: 0.000 ##  Reward 2 : 0.090 ## Max: 0.090\n",
      "Average Score of last 100 episodes: 0.007\n",
      "Episode 250 ## Reward 1 agent: 0.000 ##  Reward 2 : 0.090 ## Max: 0.090\n",
      "Average Score of last 100 episodes: 0.007\n",
      "Episode 251 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.007\n",
      "Episode 252 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.007\n",
      "Episode 253 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.007\n",
      "Episode 254 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.007\n",
      "Episode 255 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.007\n",
      "Episode 256 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.007\n",
      "Episode 257 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.007\n",
      "Episode 258 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.007\n",
      "Episode 259 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.007\n",
      "Episode 260 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.007\n",
      "Episode 261 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.007\n",
      "Episode 262 ## Reward 1 agent: 0.000 ##  Reward 2 : 0.090 ## Max: 0.090\n",
      "Average Score of last 100 episodes: 0.007\n",
      "Episode 263 ## Reward 1 agent: 0.000 ##  Reward 2 : 0.090 ## Max: 0.090\n",
      "Average Score of last 100 episodes: 0.008\n",
      "Episode 264 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.008\n",
      "Episode 265 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.008\n",
      "Episode 266 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.008\n",
      "Episode 267 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.008\n",
      "Episode 268 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.008\n",
      "Episode 269 ## Reward 1 agent: 0.000 ##  Reward 2 : 0.090 ## Max: 0.090\n",
      "Average Score of last 100 episodes: 0.009\n",
      "Episode 270 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.009\n",
      "Episode 271 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.009\n",
      "Episode 272 ## Reward 1 agent: 0.000 ##  Reward 2 : 0.090 ## Max: 0.090\n",
      "Average Score of last 100 episodes: 0.010\n",
      "Episode 273 ## Reward 1 agent: 0.000 ##  Reward 2 : 0.090 ## Max: 0.090\n",
      "Average Score of last 100 episodes: 0.011\n",
      "Episode 274 ## Reward 1 agent: 0.000 ##  Reward 2 : 0.090 ## Max: 0.090\n",
      "Average Score of last 100 episodes: 0.012\n",
      "Episode 275 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.012\n",
      "Episode 276 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.012\n",
      "Episode 277 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.012\n",
      "Episode 278 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.011\n",
      "Episode 279 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.011\n",
      "Episode 280 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.011\n",
      "Episode 281 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.011\n",
      "Episode 282 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.011\n",
      "Episode 283 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.011\n",
      "Episode 284 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.011\n",
      "Episode 285 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.011\n",
      "Episode 286 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.011\n",
      "Episode 287 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.011\n",
      "Episode 288 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 289 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.011\n",
      "Episode 290 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.011\n",
      "Episode 291 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.011\n",
      "Episode 292 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.011\n",
      "Episode 293 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.011\n",
      "Episode 294 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.011\n",
      "Episode 295 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.100 ## Max: 0.100\n",
      "Average Score of last 100 episodes: 0.012\n",
      "Episode 296 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.100 ## Max: 0.100\n",
      "Average Score of last 100 episodes: 0.013\n",
      "Episode 297 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.013\n",
      "Episode 298 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.013\n",
      "Episode 299 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.013\n",
      "Episode 300 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.013\n",
      "Episode 301 ## Reward 1 agent: 0.000 ##  Reward 2 : 0.090 ## Max: 0.090\n",
      "Average Score of last 100 episodes: 0.014\n",
      "Episode 302 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.014\n",
      "Episode 303 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.100 ## Max: 0.100\n",
      "Average Score of last 100 episodes: 0.015\n",
      "Episode 304 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.015\n",
      "Episode 305 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.100 ## Max: 0.100\n",
      "Average Score of last 100 episodes: 0.016\n",
      "Episode 306 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.016\n",
      "Episode 307 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.016\n",
      "Episode 308 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.016\n",
      "Episode 309 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.100 ## Max: 0.100\n",
      "Average Score of last 100 episodes: 0.017\n",
      "Episode 310 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.017\n",
      "Episode 311 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.100 ## Max: 0.100\n",
      "Average Score of last 100 episodes: 0.018\n",
      "Episode 312 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.018\n",
      "Episode 313 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.018\n",
      "Episode 314 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.017\n",
      "Episode 315 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.016\n",
      "Episode 316 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.016\n",
      "Episode 317 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.016\n",
      "Episode 318 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.016\n",
      "Episode 319 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.016\n",
      "Episode 320 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.016\n",
      "Episode 321 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.016\n",
      "Episode 322 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.016\n",
      "Episode 323 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.016\n",
      "Episode 324 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.016\n",
      "Episode 325 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.016\n",
      "Episode 326 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.016\n",
      "Episode 327 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.015\n",
      "Episode 328 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.015\n",
      "Episode 329 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.015\n",
      "Episode 330 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.100 ## Max: 0.100\n",
      "Average Score of last 100 episodes: 0.016\n",
      "Episode 331 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.100 ## Max: 0.100\n",
      "Average Score of last 100 episodes: 0.017\n",
      "Episode 332 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.016\n",
      "Episode 333 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.100 ## Max: 0.100\n",
      "Average Score of last 100 episodes: 0.017\n",
      "Episode 334 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.017\n",
      "Episode 335 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.017\n",
      "Episode 336 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.017\n",
      "Episode 337 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.100 ## Max: 0.100\n",
      "Average Score of last 100 episodes: 0.018\n",
      "Episode 338 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.018\n",
      "Episode 339 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.100 ## Max: 0.100\n",
      "Average Score of last 100 episodes: 0.019\n",
      "Episode 340 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.019\n",
      "Episode 341 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.100 ## Max: 0.100\n",
      "Average Score of last 100 episodes: 0.020\n",
      "Episode 342 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.020\n",
      "Episode 343 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.020\n",
      "Episode 344 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.020\n",
      "Episode 345 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.020\n",
      "Episode 346 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.100 ## Max: 0.100\n",
      "Average Score of last 100 episodes: 0.021\n",
      "Episode 347 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.021\n",
      "Episode 348 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.100 ## Max: 0.100\n",
      "Average Score of last 100 episodes: 0.022\n",
      "Episode 349 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.021\n",
      "Episode 350 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.020\n",
      "Episode 351 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.020\n",
      "Episode 352 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.020\n",
      "Episode 353 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.020\n",
      "Episode 354 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.020\n",
      "Episode 355 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.020\n",
      "Episode 356 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.020\n",
      "Episode 357 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.020\n",
      "Episode 358 ## Reward 1 agent: 0.000 ##  Reward 2 : 0.090 ## Max: 0.090\n",
      "Average Score of last 100 episodes: 0.021\n",
      "Episode 359 ## Reward 1 agent: 0.100 ##  Reward 2 : -0.010 ## Max: 0.100\n",
      "Average Score of last 100 episodes: 0.022\n",
      "Episode 360 ## Reward 1 agent: 0.000 ##  Reward 2 : 0.090 ## Max: 0.090\n",
      "Average Score of last 100 episodes: 0.023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 361 ## Reward 1 agent: 0.200 ##  Reward 2 : 0.090 ## Max: 0.200\n",
      "Average Score of last 100 episodes: 0.025\n",
      "Episode 362 ## Reward 1 agent: 0.200 ##  Reward 2 : 0.190 ## Max: 0.200\n",
      "Average Score of last 100 episodes: 0.026\n",
      "Episode 363 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.100 ## Max: 0.100\n",
      "Average Score of last 100 episodes: 0.026\n",
      "Episode 364 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.100 ## Max: 0.100\n",
      "Average Score of last 100 episodes: 0.027\n",
      "Episode 365 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.027\n",
      "Episode 366 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.027\n",
      "Episode 367 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.027\n",
      "Episode 368 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.027\n",
      "Episode 369 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.100 ## Max: 0.100\n",
      "Average Score of last 100 episodes: 0.027\n",
      "Episode 370 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.100 ## Max: 0.100\n",
      "Average Score of last 100 episodes: 0.028\n",
      "Episode 371 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.028\n",
      "Episode 372 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.100 ## Max: 0.100\n",
      "Average Score of last 100 episodes: 0.029\n",
      "Episode 373 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.028\n",
      "Episode 374 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.027\n",
      "Episode 375 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.027\n",
      "Episode 376 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.027\n",
      "Episode 377 ## Reward 1 agent: 0.000 ##  Reward 2 : -0.010 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.027\n",
      "Episode 378 ## Reward 1 agent: -0.010 ##  Reward 2 : 0.000 ## Max: 0.000\n",
      "Average Score of last 100 episodes: 0.027\n"
     ]
    }
   ],
   "source": [
    "scores = run_ddpg_multi()\n",
    "\n",
    "average_scores = []\n",
    "temp_deque = deque(maxlen=100)\n",
    "for s in scores:\n",
    "    temp_deque.append(s)\n",
    "    average_scores.append(np.mean(temp_deque))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.plot(range(1, len(scores)+1), scores)\n",
    "ax.plot(range(1, len(average_scores)+1), average_scores, c='y', label='mean over 100 episodes')\n",
    "ax.set_xlabel('Episode #', fontsize=14)\n",
    "ax.set_ylabel('Score', fontsize=14)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
